% Typeset for Transactions on Architecture and Code Optimization
\documentclass[prodmode,acmtaco]{acmsmall}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{filecontents}
%\usepackage{booktabs} % Only used for horizontal lines
\usepackage{tikz}
\usepackage{listings}
%\usepackage{caption}
\usepackage{mathtools}
\usepackage{underscore}
\usepackage{url}
\usepackage{lmodern} % Much clearer text

\usetikzlibrary{patterns,shapes,positioning,calc}

\lstset{language=C, basicstyle=\ttfamily\small, columns=flexible}

\pgfplotsset{compat=1.8}

\newcommand{\perfctr}[1] {
  {\lowercase{#1}}
}

%\acmVolume{V}
%\acmNumber{N}
%\acmArticle{A}
%\acmYear{YYYY}
%\acmMonth{0}

% Page headers
\markboth{L. K. Melhus and R. E. Jensen}{Measurement Bias from Address Aliasing}

\title{Measurement Bias from Address Aliasing}
\author{
LARS KIRKHOLT MELHUS and RUNE ERLEND JENSEN
  \affil{Norwegian University of Science and Technology}}

% Motivation -- Problem statement -- Approach -- Results -- Conclusions
\begin{abstract} % Should be 150 -- 200 words max
%Understanding program performance and obtaining accure measurements is important in performance analysis, but measurements can be \emph{biased} by external factors.
%Predicting or accounting for measurment bias is challenging, as the interaction with low level hardware mechanisms is complex.  
%In this paper we show how \emph{address aliasing} can be the underlying mechanism resulting in measurement bias on modern Intel microarchitectures.
%Our approach is to use hardware performance counter to analyze performance data over controlled series of execution contexts.
%We show how address aliasing can explain bias from two external factors: environment variable size, and characteristics of dynamically linked heap allocators.

%The perfomance impact of unfavorable memory contexts can be significant, with as much as $2x$ speedup in one of our examples.
%It also turns out the allocation pattern of common libraries tends to give \emph{worst case} alignment by default, by favoring page alignment large allocations.
%While the aliasing mechanism itself is not new, we provide the connection to measurement bias and show how to use this knowledge not only to account for bias but also explicitly optimize for it.
%Our results should provide motivation for more architecture specific optimization, for example in heap allocator heuristics.
Understanding program performance and obtaining accurate measurements is important in performance analysis, but measurements can be \emph{biased} by external factors.
It has been shown that there can be significant performance differences based seemingly irrelevant factors like user name length.
In this paper we show an important underlying mechanism resulting in this type of measurement bias on modern Intel microarchitectures.

Our approach is to search for correlations between a large set of hardware performance counters and performance figures from multiple environment contexts.
We find that \emph{address aliasing} can explain bias from two external factors: environment variable size, and characteristics of dynamically linked heap allocators.

This clear result not only makes accounting and avoiding measurement bias much simpler, but enables explicit optimizations for it.
It also turns out the allocation pattern of common libraries tends to give \emph{worst case} alignment by default, by favoring page alignment large allocations.
The performance impact of unfavorable memory contexts can be significant, with as much as $2x$ speedup in one of our examples.
Our results should provide motivation for more architecture specific optimization, for example in heap allocator heuristics and compiler rules.

\end{abstract}

% Choose from classification tree http://www.acm.org/about/class/ccs98-html
\category{C.4}{Performance of Systems}{Measurement techniques; Performance attributes}

% Terms come from a fixed list of predefined terms
\terms{Measurement, Performance}

% Free to choose keywords, but must be in alphabetical order
\keywords{4K address aliasing, heap allocators, measurement bias, memory disambiguation}

% Minimum is author, year and title
\acmformat{Lars K. Melhus and Rune E. Jensen. 2014. Measurement Bias from Address Aliasing}


\begin{document}

% Acknowledgement of support, author(s) address(es)
\begin{bottomstuff}
\end{bottomstuff}

\maketitle


\section{Introduction}
Accurately measuring the performance of computer programs is important in order to evaluate new algorithms or compare different implementations.
This process is complicated by \emph{measurment bias}, which can be a challenge for performance analysist and systems researchers.
Changes to external factors of the system, such as link ordering or the contents of environment variables, has been shown to have potentially significant impacts on performance in real applications~\cite{Mytkowicz:2009:WrongData}.
The reason this happens is because certain variables has the potential to offset memory addresses of code or data, which in turn interacts with various low level hardware buffers at runtime.
Because of the complexity of modern processors, it is often hard or even impossible to predict exactly how changes to things like system environment variables ultimately affect program execution.

To alleviate the impact of measurement bias in performance analysis, researchers have proposed to use causal analysis techniques and randomization of execution contexts~\cite{Mytkowicz:2008:OE&MB}.
Others treat the existence bias as a potential optimization problem, using search over variant spaces to find optimal environments~\cite{Knights:2009:BlindOpt}.
Our approach is to analyze isolated programs in detail using \emph{performance counters}, in an attempt to identify which underlying hardware mechanisms are causing bias.

In this paper we show how some instances of measurement bias can be explained by \emph{address aliasing}, an artifact of a presumably Intel specific optimization on the out-of-order execution pipeline.
The CPU uses a heuristic for determining whether loads are dependent on previous stores, comparing only the last 12 virtual address bits.
False dependencies can happen for aliased memory accesses, and incur performance penalties.
We look at how address aliasing can trigger bias from two external factors; size of system environment variables and configuration of dynamic heap allocation libraries.

Our results show that the cost of aliasing can be significant, exemplified by a simple program with more than $2x$ speedup based in heap address alignment alone.
We also find that many heap allocation libraries tend to produce worst case behavior by default with respect to aliasing, by favoring page alignment for large allocations.
With an understanding of how false dependencies in the CPU can impact performance, measurement bias caused by it can to some extent be predicted and even accounted for in software.
We show how techniques like manual padding of allocations and dynamic detection of aliasing conditions can be used to improve performance. 
Accounting for address aliasing can give significant speedups, and our findings should motivate to exploit this in architecture-specific optimization.

The rest of this paper is structured as follows: Our methodology and experimental setup is explained in the next section, then section \ref{sec:aliasing} introduces ``4K aliasing'', providing necessary background for interpreting the later results.
Section \ref{sec:environment} presents an analysis of how the size of environment variables causes bias.
In Section \ref{sec:heap} we go through a similar analysis for bias to heap address alignment.
Related work is discussed in Section \ref{sec:related}.
Section \ref{sec:conclusions} includes a summary and conclusion.


\section{Experimental Methodology}
\label{sec:methodology}
In order to understand biased behavior it is important to be able to do precise and detailed measurements, without introducing \emph{observer effects}~\cite{Mytkowicz:2008:OE&MB}.
This can be achieved by using performance counters, instrumentation support in hardware that can be used to \emph{count} various events, such as cycles executed, branch misses, instructions fetched, and so on.
Recent Intel architectures have several hundred available events, providing a detailed view of what happens inside the CPU.
Performance counters are supported in the Linux kernel, accessed via a tool called \emph{perf}.
We use the perf-stat command in all experiments, which accepts raw event codes listed in the reference manual~\cite{Volume3B}.

A small Python script is used to collect an exhaustive set of all available counters, which amounts to about 200 on our architecture.
Only a small set of events are collected at a time, to ensure events are actually counted continuously and not sampled by multiplexing between a limited set of counter registers.
Controlled variations in environment size is performed by setting a dummy environment variable to $n$ number of zero characters, starting from a minimal environment.\footnote{Because perf-stat itself adds a few variables, the environment will never be completely empty.}
Interesting events are identified by computing linear correlation to cycle count, measuring all counters over a series of execution contexts.
% Cache related metrics are monitored in order to rule out cache as the underlying cause of bias, such as hit rates of micro-ops for each level of cache~\cite{OptimizationManual}.
Results are averaged over multiple runs to reduce potential random error, supplying the -r option to perf-stat.

Because bias can be hardware dependent, and to keep the scope of this work manageable, we choose to focus on the Intel ``Haswell'' microarchitecture specifically.
Our setup consists of a 4th generation Intel\textsuperscript{\textregistered{}} Core\texttrademark{} i7-4770K processor, running 64-bit Ubuntu 14.04 LTS on kernel version 3.13.0-24-generic.
Code samples are compiled using the {\small GCC} toolchain, version 4.8.2-19ubuntu1.

We also have to ensure we are not affected by measurement bias beyond what we are actually trying to observe, and in general follow best practices on gathering data~\cite{Mytkowicz:2009:WrongData}.
Most importantly, this means keeping the memory address space under control.
For security reasons, addresses of stack, heap and dynamic libraries are often randomized at load time, a technique known as \emph{Address Space Layout Randomization}~\cite{Pax:ASLR,Bhatkar:AddressObfuscation}. 
By disabling ASLR, we are able to execute the same program multiple times with identical virtual address spaces.
All experiments are done on a machine under minimal load, and \emph{frequency scaling} is disabled to keep the CPU's clock speed fixed.
Finally, we disable \emph{Hyper-threading}, which reduces the possibility for resource contention between threads.


\section{4K Address Aliasing}
\label{sec:aliasing}
Modern processors are \emph{superscalar}, and achieves parallelism by issuing multiple instructions simultaneously and out of order.
One of the issues that can limit throughput is dependencies between a load and previous stores.
% According to Intel, typical software consists of about 38~\% memory access instructions, with about two thirds of them being loads.
To increase parallelism, modern architectures use a technique called \emph{memory disambiguation} to execute memory operations out of order~\cite{Intel:2006:InsideICM:SmartMemoryAccess}. 
More often than not, loads can safely be issued before a previous store has completed and written its value to L1 cache.
Loads are therefore issued \emph{speculatively}, based on a prediction on whether it will conflict with a previous store that is still not retired.
The prediction is later verified, replaying any instructions that was wrongly assumed to have no dependencies.
Similarly, if the load and store locations are the same, the value can be \emph{forwarded} from the store before it retires.

While optimizations such as these are good on the average case, there are corner cases. 
In particular, an event known as ``4K aliasing'' can occur when the memory addresses of a store followed by a load differ by a multiple of 4096.
A store to address 0x601020 followed by a load to address 0x821020 is an aliasing pair, because the 12-bit address suffix of 0x020 is the same in both. 
Despite being independent, in these cases the memory order subsystem generates \emph{false} dependencies, and causing the load to be reissued.
The number of times this happens can be counted by the following performance counter:
\begin{description}
  \item[{\small LD\_BLOCKS\_PARTIAL.ADDRESS\_ALIAS}] ``Counts the number of loads that have partial address match with preceding stores, causing the load to be reissued.'' Intel Optimization Manual~\citeyear[B.3.4.4]{OptimizationManual}
\end{description}

As address aliasing depends on the memory addresses of loads and stores, environmental factors that affects memory has the potential to induce aliasing conditions.
In the following sections, we show how performance penalties from address aliasing can be the root cause of measurement bias.

\begin{narrowfig}{0.5\textwidth}[t]
  \centering
  \begin{tikzpicture}[font=\footnotesize]
    % See page 453 in pgf manual
    \node [
      rectangle split, rectangle split parts=10, 
      rectangle split part fill={white, white, lightgray, white, lightgray, white, lightgray, white},
      draw, anchor=center, text width=2cm
    ] (m)
      {
        \nodepart{one}
          environment
        \nodepart{two}
          stack
        \nodepart{four}
          mmap area
        \nodepart{six}
          heap
        \nodepart{eight}
          bss
        \nodepart{nine}
          data
        \nodepart{ten}
          text
      };
    \draw [decorate, decoration={brace, amplitude=5pt}] (m.south west) -- (m.seven split west) 
      node [black, midway, xshift=-1.5cm, text width=2.0cm] 
        { Program code and static data } ;
    \node[right] at (m.north east)
      { \scriptsize{0x7fff'ffffffff} } ;
    \node[right] at (m.south east)
      { \scriptsize{0x400000} } ;
  \end{tikzpicture}
  \caption{Relative positions of some important sections of memory at runtime, assuming a 64-bit process mapped to virtual memory.
  Initial addresses of stack, heap and memory mapped files are often randomized for security reasons.
  Addresses of code and statically allocated data are allocated at compile time by the linker, and can be determined by inspecting the executable.}
  \label{fig:virtualmemory}
\end{narrowfig}


\section{Bias from Environment Size}
\label{sec:environment}
Unless the program explicitly accesses environment variables, it is not the environment variables themselves that are important, but rather the effect their total size has on the position and alignment of stack.
As indicated in Figure \ref{fig:virtualmemory}, environment variables and program arguments are allocated in the stack section of virtual memory close to the upper address 0x7fff'ffffffff\footnote{Modern processors do not actually use the full 64-bit space, only the low order 47 bits are used for addressing memory}, before the first call frame.
Changing environment variables will therefore offset the addresses of stack, and consequently all stack allocated variables.
After some variable amount of offset, the stack is normally aligned to a 16 byte boundary, which will be enforced by the compiler.
Within a span of 4096 bytes there are thus 256 possible initial stack addresses, each representing a different execution context with respect to address aliasing.\footnote{Note that there is no clear relationship between environment size and stack location with ASLR enabled.
However, there will still be as many execution contexts with respect to aliasing (considering stack only), making any occurrences of measurement bias indeed random.}

\subsection{Microkernel Analysis}
To illustrate how address aliasing can cause bias, we revisit the example first presented in ``Producing Wrong Data Without Doing Anything Obviously Wrong!'' by \citeN{Mytkowicz:2009:WrongData}, reproduced below. 
This example is interesting for several reasons; the bias effects are significant and easily reproducible, while the example code is simple and straightforward to analyze.
Still, no satisfactory explanation as to what actually causes bias was given in the original paper.

\begin{lstlisting}
    static int i, j, k;
    int main() {
        int g = 0, inc = 1;
        for (; g < 65536; g++) {
            i += inc;
            j += inc;
            k += inc; 
        }
        return 0;
    }
\end{lstlisting}

% Large plot of cycle count for two 4K stack sections.
\pgfplotstableread{bin/micro-kernel-cycles.dat}{\stackoffsettable}
\begin{figure*}[t]
  \begin{tikzpicture}
    \begin{axis}[
        width = \textwidth,
        height = 6cm,
        font = \footnotesize,
        xlabel=Bytes added to environment,
        ylabel=Cycles,
        domain = 0:8192,
        xtick = {0,1024,...,8192},
        xmin = 0,
        xmax = 8192,
        cycle list name = exotic
      ]
      \addplot[ycomb] table[x expr = \thisrowno{0}*16, y = cycles:u] \stackoffsettable ;
    \end{axis}
  \end{tikzpicture}
  \caption{Bias from environment size for microkernel. Measured average of 10 cycle count samples for 512 different environments. Spikes show aliasing case, occurring once for each 4K period}
  \label{fig:envbias}
\end{figure*}

Performance counter measurements of cycle counts are shown for 512 different environment sizes in Figure~\ref{fig:envbias}.
We measure every 16 byte increment of environment size, covering two 4K periods of initial stack addresses.
A finer sampling is not necessary, because the stack is by default aligned to 16 byte.
The program is compiled with {\small{GCC}} using no optimization, as any optimization would likely disregard most of the function as redundant code, reducing it to return zero immediately.

There are clearly two worst cases, indicated by significant spikes at the 3184 and 7280 byte offsets.
We sampled an extensive set of performance counters in addition to cycle count for each execution context.
By looking at the \emph{median} value of each counter compared to the extreme cases, we narrowed down the performance events that behaved similarly to cycle count.
Table \ref{tab:loopcorrelation} shows numerical counter values for worst cases, compared to the median over all environments.

% Detailed comparison between median and worst cases.
\begin{table}
  \tbl{Events with significant correlation to cycle count\label{tab:loopcorrelation}}{
    \pgfplotstabletypeset[
      int detect, % Output whole numbers for counter values
      col sep=comma,
      columns={Performance counter, Median, [index]3, [index]4},
      column type=r,
      columns/Performance counter/.style={
        string type, 
        column type=l,
        column type/.add={|}{},
        postproc cell content/.append code={
          \pgfkeysalso{@cell content=\perfctr{##1}}
        }
      },
      every head row/.style={
        output empty row,
        before row={\hline
          Performance counter & Median & Spike 1 & Spike 2 \\
        },
        after row=\hline\hline
      },
      every last row/.style={after row=\hline},
      every last column/.style={column type/.add={}{|}}
    ]{bin/micro-kernel-comparison.csv}
  }
  \begin{tabnote}
    \Note{Note:}{Performance events that are obviously not indicative of any causal relationship are omitted, for example bus-cycles which will naturally vary with total cycle count.}
  \end{tabnote}
\end{table}

We see the most extreme change from median to worst case is clearly the number of alias events.
If we plot the graph for address aliasing, we see that it is near zero everywhere and spikes at exactly the points we observe bias. 
The results show a high number of resource stalls and pending memory loads when the spikes occur, which is consistent with address aliasing issues.
On the other end we get a much lower number of reservation station (RS) stalls in the aliasing case, with a reduction from about 270,000 to about 135,000.
The reservation station buffers micro-ops for scheduling to the execution units, and a stall event means that there are no free slots available~\cite[Table 19-2]{Volume3B}.
Fewer stalls in the aliasing case could indicate less contention on the reservation station.
This probably has to do with the overall \emph{decrease} in the number of micro-ops executed per port, as all but one of the in total 8 execution ports seem to have fewer operations going in parallel.
Note that the number of micro-ops \emph{retired} overall does not change, but for some reason the micro-ops stays in the execution pipeline for a shorter time.
Our assumption is that this is a by-product of stalls generated from aliasing, causing micro-operations to be reissued.

Performance counter data clearly points to address aliasing as a plausible explanation, thus the next step is to see exactly which memory accesses are aliasing. 
For that we need to know the addresses of each variable at runtime.
The program contains five variables; \texttt{g} and \texttt{inc} which are stack allocated, and \texttt{i}, \texttt{j} and \texttt{k} which are statically allocated.
Virtual memory layout of static data is decided at compile time, and we can find the location of these variables by looking at the symbol table in the ELF executable.
We find the addresses to be \texttt{\&i} = 0x60103c, \texttt{\&j} = 0x601040, and \texttt{\&k} = 0x601044.\footnote{ELF symbol tables can be read using \texttt{readelf -s}}

Observing addresses of stack allocated data at runtime is more challenging, as we have to make sure to not introduce any observer effects that alters the addresses as we are observing them.
A small amount of assembly code was added to calculate the addresses of \texttt{g} and \texttt{inc}, outputting to stdout directly using the \texttt{syscall} instruction.
Making no change to the stack allocation instructions and not affecting the addresses of static variables, the instrumented program have the exact same bias to environment size, free from observer effects.
We find that the spike in cycle count occurs precisely when the address of \texttt{inc} alias with \texttt{i}, the first spike happening when \texttt{g} is at 0x7fffffffe{\underline{038} and \texttt{inc} is at 0x7fffffffe{\underline{03c}}.
The assembly output of {\small GCC} for the inner loop is shown below, indicating the relevant load and store instructions.
There is only one store to variable \texttt{i}, which will alias with loads of \texttt{inc}.

% Refer to relevant section of the generated assembly file (in gas syntax)
\lstinputlisting[
  basicstyle=\ttfamily\small,
  linerange={22-38}, 
  numbers=left,
  firstnumber=22
  ]{bin/micro-kernel-annotated.s}

Because the stack is aligned to 16 bytes, or 4 \emph{words}, there are a couple of different scenarios that could have been observed here.
Static variables are fixed and covers 12 contiguous bytes (3 words), in our case the addresses end in 0x0, 0x4 and 0xc, leaving the 0x8 slot free.
Automatic variables occupies 8 contiguous bytes on stack (2 words), in our case the addresses will always fit in the 0x8 and 0xc slots.
In this scenario, \texttt{g} will never alias with any of the static variables -- as it always covers the 0x8 slot not occupied by either of \texttt{i}, \texttt{j} or \texttt{k}.
A less fortunate scenario with respect to the number of alias events occurs when there can be collisions with both stack allocated variables, which can be achieved for example by reserving an extra 8 bytes to offset \texttt{i}, \texttt{j} into the 0x8, 0xc slots. 
While this will give significantly more alias counts, it has little effect on the total number of cycles executed.

In conclusion, we identified that address aliasing is the root cause of measurement bias from environment size for this program.
Worst case occurs for precisely one out of 256 possible initial stack addresses in every 4K segment, where resource stalls are generated because of false dependencies between stack and static data.
The program is \emph{biased} towards environment sizes that avoids this specific stack alignment.


\subsection{Avoiding Aliasing}
Addresses of automatic variables can not be determined statically, because the position of stack at runtime is generally unknown. 
In addition to being offset by environment variables, the stack address can also be perturbed by other factors such as ASLR or program arguments. 
Although one can not easily know if a collision is going to happen for a given environment, it is possible to change the program to account for possible alias effects.
Shown in Figure \ref{lst:loopfixed} is a proof of concept of how alias-free code could be generated in this particular case.
If the addresses do alias, a branch to an alternative but semantically equivalent code path is performed.
Calling the function recursively will effectively allocate a new set of variables a bit further down the stack, and the alias condition is avoided.

% See analysis for results showing this does not alias
\begin{figure}
  \begin{lstlisting}[frame=single, xleftmargin=.1\textwidth, xrightmargin=.1\textwidth]
#define ALIAS(a, b) \
    (((long)&a) & 0xfff == ((long)&b) & 0xfff)
static int i, j, k;
int main() {
    int g = 0, inc = 1;
    if (ALIAS(inc, i) || ALIAS(g, i))
        return main();
    for (; g < 65536; g++) {
        i += inc;
        j += inc;
        k += inc;
    }
    return 0;
}
  \end{lstlisting}
  \caption{Dynamically detect aliasing case, and avoid by pushing another stack frame.}
  \label{lst:loopfixed}
\end{figure}

While this particular solution is clearly not something one would want to do in practice, it at least shows that compilers and programmers \emph{could} take measures to account for aliasing.
This becomes much more enticing when the performance penalties are more significant, which we find is often the case for aliasing heap allocations.


\section{Bias from Heap Allocation}
\label{sec:heap}
Address aliasing can be caused by conflicting pairs of load and store operations to any part of memory.
In the previous sections we saw collisions between static data and stack, observing bias from external conditions that affected addresses of automatic variables.
Most dynamic memory is allocated on the \emph{heap}, which is managed by an \emph{allocator}.
A heap allocator is responsible for managing dynamic memory, and ultimately assigns the actual addresses of heap allocated variables at runtime.
Heap allocation routines such as \texttt{malloc} and \texttt{free} are typically dynamically linked, for example as part of glibc.
The particular library used therefore constitutes an important part of the execution context, as linking to a different library, or a library with some alternative configuration, can impact heap addresses at runtime.


\subsection{Most Allocators Alias by Default}
Acquiring dynamic memory at runtime is usually done by calling \texttt{malloc}, which takes a number of bytes to allocate as input, and returns a pointer to that area.
Depending on the particular request and allocator used, the returned value will either point to the ``regular'' heap, or to a memory mapped area (see Figure \ref{fig:virtualmemory}).
Different strategies for how to allocate memory will often depend on the \emph{size} of the request.

\begin{enumerate}
  \item The heap is marked by a brake point representing the end of uninitialized data in virtual memory, and more space is requested by the \texttt{brk} or \texttt{sbrk} system calls. 
  Allocators typically use this area for smaller allocations.
  \item Anonymous memory mappings, i.e. buffers not backed by a file, can be created through the \texttt{mmap} system call.
  This mechanism is typically used for large allocations.
\end{enumerate}

The heap section starts at a relatively low address right above static code and data.
Memory mapped chunks are placed towards the upper end of the virtual address space, closer to stack.
Whether a memory allocation request is served by the heap or by memory mapping is therefore easy to determine just by looking at the pointer values returned:
While addresses in the regular heap can look something like 0x16e30a0 or 0x1723020, pointers returned by \texttt{mmap} are numerically much larger, for example 0x7f0318a8f010 or 0x7f03105d2010.
This distinction is of course unimportant for application developers, as everything is conceptually the same ``heap''.
However, \texttt{mmap} has an interesting property in that allocations will always be page aligned.
The page size is 4096 bytes, meaning two pointers returned by \texttt{mmap} will \emph{always} alias.\footnote{glibc's version of malloc adds 16 bytes of metadata at the beginning, therefore every memory mapped address ends with 0x010.}
This behavior is often the worst case for functions that operate on two or more independent buffers, as we will see in the following sections.

Table \ref{tab:mallocompare} illustrates how different dynamic allocators tend to produce aliasing conditions for large requests\footnote{Switching allocator was done by setting the LD\_PRELOAD environment variable.}.
The addresses of two equally sized \texttt{char} buffers allocated with \texttt{malloc} are observed for different size parameters, and equal three digit address suffix indicate an aliasing pair.
In addition to glibc, where the heap allocator is called \emph{ptmalloc}, we look at the following alternatives:
\begin{itemize}
  \item Thread-Caching Malloc (tcmalloc) by Google~\cite{TCMalloc}.
  \item jemalloc, originally developed for FreeBSD~\cite{JEMalloc}.
  \item Hoard~\cite{Berger:2000:Hoard}.
\end{itemize}
All of the above focuses heavily on performance in multithreaded environments.
Heap allocation is intrinsically inefficient in that all threads share the same address space, leading to a high potential lock contention on memory accesses.
Here we only look at behavior for a single thread, and whether the addresses returned alias or not.
\begin{table}[t]
  \label{tab:mallocompare}
  \tbl{Addresses returned by different heap allocators when allocating pairs of equally sized buffers.}{
    \pgfplotstabletypeset[
      col sep=comma,
      string type,
      columns={Allocation, 64, 5120, 1048576},
      column type=r,
      columns/Allocation/.style={
        string type, 
        column type=l,
        column type/.add={|}{}
      },
      every head row/.style={
        output empty row,
        before row={\hline
           & 64 B & 5,120 B & 1,048,576 B \\
          },
        after row=\hline\hline,
      },
      every last row/.style={after row=\hline},
      every last column/.style={column type/.add={}{|}},
      every odd row/.style={after row=\hline},
    ]{bin/malloc-comparison.csv}
  }
  \begin{tabnote}
    \Note{Note:}{Memory mapped addresses starting with the 0x2aa... prefix is an artifact of using \emph{make} to generate results. Executing the test program from \emph{bash} directly result in 0x7fff... prefixes.
    The difference is not important to our discussion.}
  \end{tabnote}
\end{table}

For smaller allocations, we see that glibc and tcmalloc utilize the normal heap area -- returning numerically low addresses.
Interestingly, jemalloc and Hoard appears to never use the heap, but allocate to memory mapped areas even for smaller requests.
Conversely, tcmalloc seem manage only the heap.
We see one example where one allocator yields aliasing buffers while another does not.
Allocating $2 \times 5120$ bytes returns aliasing pointers for jemalloc and Hoard, but not with glibc or tcmalloc.
Given that these results are deterministic (with ASLR disabled), it is not hard to construct a program with significant bias towards one or the other allocator.
But even with randomization, addresses returned by \texttt{mmap} must still be page aligned.
This means that addresses returned by allocators using this mechanism directly will \emph{always} alias, giving a deterministic execution context considering only the address suffix.
From our limited experiment, this seems to be the case for glibc, jemalloc and Hoard.


\subsection{Aligned Sequential Access}
Many functions operate in a ``sliding window'' fashion; reading and writing to different buffers in some loop construction.
This type of program is potentially vulnerable to 4K aliasing, where the worst case will be when the read and write pointer addresses are aliased, continuously generating false conflicts.
As an example of this, consider a naive implementation of linear correlation shown in Figure \ref{lst:conv}.
The function contains lots of interleaved loads and stores, iterating over two independent memory buffers in a tight loop.
We will see that the performance of this program greatly depends on the address alignment of each buffer, favoring memory addresses that are not closely aligned on the last 12-bits.

\begin{figure}[t]
  \centering
  \lstinputlisting[
    language=C,
    frame=single,
    xleftmargin=.1\textwidth,
    xrightmargin=.1\textwidth
  ]{bin/convolution-kernel.c}
  \caption{Naive implementation of convolution, ignoring endpoints for simplicity. This program is highly sensitive to aliasing between input and output arrays.}
  \label{lst:conv}
\end{figure}

By using an input size of $n=2^{20}$ (4 MiB in memory for each array), glibc's heap allocator ends up always choosing \texttt{mmap} to serve requests.
By default, even with address randomization enabled, both pointers will have the same address suffix of 0x010.
To analyse performance for different addresses, we manually insert padding to offset one of the memory mapped pointers.
This is accomplished by requesting a bit more memory, and use pointer arithmetic to offset one of the function arguments.
Controlling the offset parameter, we can create environments where the inputs are some number of \texttt{sizeof(float)} bytes apart.
Allocating and managing these buffers at startup takes a non-neglegible amount of work.
The overhead can be masked by repeatedly invoking the convolution kernel after allocating and initializing all the inputs.
\begin{lstlisting}
    for (i = 0; i < k; ++i)
        conv(n, input, output + offset);
\end{lstlisting}
An estimate of the actual cost of one invocation can be calculated by averaging over a number of repeated function calls, after subtracting the constant overhead of invoking it only once.
\[
t_{\text{estimate}} = \frac{t_{k} - t_{1}}{k - 1}
\]
Our results are with $k=11$, effectively using the average over 10 loop iterations to estimate the cost of a single invocation.
For every run, performance counter measurements is also averaged over 10 samples, using the repeat mechanism of perf.

\pgfplotstableread{bin/conv-default-o2.estimate.dat}{\convtabletwo}
\pgfplotstableread{bin/conv-default-o3.estimate.dat}{\convtablethree}
\begin{figure*}[t]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        title=\texttt{cc -O2},
        font=\footnotesize,
        xlabel=Relative offset in \lstinline!sizeof(float)! bytes,
        cycle list name=black white,
        width=\textwidth/1.9, % Make it fit to text width side by side
        skip coords between index={20}{32} % Limit level of detail to fit page width nicely
      ]
      \addplot table[x expr = \thisrowno{0}, y = cycles:u] \convtabletwo ;
      \addplot table[x expr = \thisrowno{0}, y = r0107:u ] \convtabletwo ;
      \addlegendentry{Cycles} ;
      \addlegendentry{Alias} ;
    \end{axis}
  \end{tikzpicture}
  \begin{tikzpicture}
    \begin{axis}[
        title=\texttt{cc -O3},
        font=\footnotesize,
        xlabel=Relative offset in \lstinline!sizeof(float)! bytes,
        cycle list name=black white,
        width=\textwidth/1.9,
        skip coords between index={20}{32}
      ]
      \addplot table[x expr = \thisrowno{0}, y = cycles:u] \convtablethree ;
      \addplot table[x expr = \thisrowno{0}, y = r0107:u ] \convtablethree ;
      \addlegendentry{Cycles} ;
      \addlegendentry{Alias} ;
    \end{axis}
  \end{tikzpicture}
  \label{fig:conv-default}
  \caption{Estimated cycle- and alias counts for different offsets between input and output arrays in convolution kernel from Figure~\ref{lst:conv}. Offset 0 means equal 12-bit address suffix, which is default behavior for \texttt{mmap} allocations. Input size $n=2^{20}$.}
\end{figure*}

% Table with detailed performance data, use estimated for O2 because it does not need as many columns (!)
\begin{table}[t]
  \renewcommand{\tabcolsep}{3pt} % Less padding between cells
  \tbl{Relevant performance counters and correlation ($r$) with cycle count for optimization O2. Estimated cost accounting for constant overhead.\label{tab:convstats}}{
    \pgfplotstabletypeset[
      int detect, % Output whole numbers for counter values
      col sep=comma,
      columns={Performance counter, Correlation, 0, 2, 4, 8},
      column type=r,
      columns/Performance counter/.style={
        string type, 
        column type=l,
        column type/.add={|}{},
        postproc cell content/.append code={
          \pgfkeysalso{@cell content=\perfctr{##1}}
        }
      },
      columns/Correlation/.style={
        fixed,
        fixed zerofill,
        precision=2
      },
      every head row/.style={
        output empty row,
        before row={\hline
          Performance counter & $r$ & 0 & 2 & 4 & 8 \\
        },
        after row=\hline\hline
      },
      every last row/.style={after row=\hline},
      every last column/.style={column type/.add={}{|}}
    ]{bin/conv-default-o2.estimate.csv}
  }
\end{table}

Figure~\ref{fig:conv-default} illustrates how the convolution kernel behaves for increasing offsets between heap addresses (modulo 4096), clearly indicating a relationship between address aliasing events and cycles executed.
The effects are most distinct on optimization level 2 and 3, where the ratio of cycles to alias events is most significant.
Numbers on the x axis represent the amount of offset, measured in number of \texttt{sizeof(float)} bytes.
An offset of zero is the default behavior for this program when using \texttt{malloc} and moderately large inputs.
Both for optimization level 2 and 3, the default alignment is close to worst case performance.
The differences in cycles executed is significant, with about $1.7x$ speedup for O2 and as much as $2x$ speedup for O3 for increasing relative offset.
This phenomenon is only observed for address offsets close to zero, thus we only show the first 20 data points.
If extended to cover the full width of possible offsets within a 4K segment, we see that the performance is uniform everywhere else.

More detailed performance counter data is presented in Table~\ref{tab:convstats}, where we have selected a subset that seem to correlate well with the change to cycle count.
The numbers are for the level 2 optimization case, which was chosen because the behavior was less chaotic and thus more easily represented by only a few numbers.
However, the performance data looks fairly similar between the two data sets, with a couple of events that seem to stand out:
\begin{itemize}
    \item A high number of resource stalls for the default alignment, which is reduced substantially together with increasing offsets. 
    \item A high number of cycles with memory loads pending, indicating that the pipeline is stalled waiting for load operations to resolve.
    \item Changes to the number of micro-ops executed for certain ports.
\end{itemize}
Interestingly, it looks like small address offsets incur a massive increase in operations issued on port 0 for the O2 case. 
On Haswell, this port handles various ALU operations, and branching together with port 6.~\cite[Figure 2.1]{OptimizationManual}
As there is also variations in the number of branch instructions executed, it seems like these counters together show that certain branch micro-ops are being re-issued.

It is worth noticing that most cache related metrics does \emph{not} stand out in this experiment.
The L1 hit rate remains stable across all offsets, and only a neglible number of memory loads actually misses L1.
On the other hand, we see a fairly strong correlation between cycle count and outstanding offcore requests. 
These events count the number of outstanding loads to memory outside the processor core each cycle.
Since we do not see any significant number of loads missing L1, the correlation to outstanding loads is probably a result of stalling and more cycles executed.

Overall it seems reasonable to conclude that the resource stalling is causing delayed execution, ultimately generated by false dependencies from address aliasing.
Variations in execution port activity, and in this case increased number of branches executed, is consistent with micro-ops being reissued after the potential conflict is detected.
We do not attempt to pinpoint exactly which assembly instructions might alias, but on a high level we can assume that the processor thinks memory accesses to \texttt{input[i]} potentially conflicts with \texttt{output[i]}.
By manually adjusting the address alignment of one of these buffers, cycle count can be reduced by as much as 50\%.
This is a speedup on top of already aggressive compiler optimization.


\subsection{Ways to Deal with Heap Address Aliasing}
Measurement bias from address aliasing can be significant in real applications, with potentially large performance penalties.
However, with an understanding of the underlying mechanism that causes bias, the effects can to some extent be predicted and accounted for in software.
The most relevant scenario to consider is probably code that can hit the aliasing \texttt{mmap} scenario, where performance impact can be consistently bad over all environments.

\paragraph{Mark buffers with \texttt{restrict}}
In our convolution kernel implementation, the compiler has to account for the fact that input and output pointers might alias, or that the buffers partially overlap.
This limits the extent generated code can keep data in registers without updating the values from memory, as a write to one buffer potentially could invalidate a cached value from the other.
The C99 keyword \emph{restrict} can be used to explicitly tell the compiler that accesses through a pointer does not alias with any other, allowing for more efficient code generation with fewer memory accesses.
\begin{lstlisting}[breaklines=true]
    void conv(int n, const float * restrict input, float * restrict output);
\end{lstlisting}
With this updated function prototype, the number of alias events is reduced by about 10 million on optimization level O2 for the default alignment, with a corresponding improvement in cycle count. % File backing this in results

\paragraph{Use a special purpose allocator}
We found that heap allocators are prone to generate pairwise aliasing buffers, the worst case for functions such as the convolution example.
A potential solution could be to apply some heuristic to randomize addresses more, and in particular not always return the same 12 bit suffix for large allocations.
This goes somewhat agains conventional wisdom that more alignment is better.
The Intel optimization manual actually mentiones this in \emph{User/Source Coding Rule 8}, suggesting that special purpose allocators could be used to avoid aliasing ~\cite{OptimizationManual}.
However, to our knowledge there are no commonly used allocators that specifically tries to mitigate aliasing in heap allocated memory.

\paragraph{Manually adjust address offsets}
In some cases it might make sense to explicitly control the memory addresses used, for example forcing some fixed relative offset between input and output pointers.
This can be achieved by exploiting the fact the \texttt{mmap} is in fact guaranteed to be placed at a page boundary, and use that directly instead of \texttt{malloc}.
The following approach can be used to make an anonymous memory mapping with offset \texttt{d} bytes away from page alignment.
\begin{lstlisting}[breaklines=true]
    mmap(NULL, (n + d), PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0) + d;
\end{lstlisting}
The same pointer difference must of course by subtracted before unmapping the memory.


\section{Related Work}
\label{sec:related}
Measurement bias and observer effect in performance analysis has been studied in detail by Mytkowicz et al~\shortcite{Mytkowicz:2008:OE&MB,Mytkowicz:2008:Easy}.
The authors introduce environment variables and link ordering as examples of external bias triggers, showing how standardised {\small SPECint} benchmarks suffer from measurement bias when trying to evaluate the effectiveness of O3 over O2. 
Strategies for detecting bias through causal analysis and randomization of experimental setups is introduced.
The microkernel example we analyzed is taken from a followup paper by the same authors, presenting similar results for an older Core 2 processor~\cite{Mytkowicz:2009:WrongData}.
Assuming the root causes of measurement bias were the same, our results indicate that address aliasing issues is probably relevant on several previous generations of Intel architectures as well.

Techniques for ``Blind'' optimization using search over variant spaces, is introduced by \citeN{Knights:2009:BlindOpt}.
Their space of variants consist of different position and alignments for each function and global variable, in principle exploring the different configurations possible by altering link order.
The authors of MAO present a framework for extending compilers to provide very low level microarchitectural assembly optimization~\cite{Hundt:2011:MAO}.
Different techniques are used, including rules, pattern matching, and random insertion of nop-instructions. 


\section{Conclusions}
\label{sec:conclusions}
In this paper we have shown how address aliasing can affect program performance under different memory layouts, and how it can explain certain cases of measurement bias. 
The effect is caused by the way speculative and out-of-order memory operations are handled by the CPU, only considering the last 12 address bits to resolve conflicts between load and store operations.

In general, any change to virtual memory layout of data can potentially introduce bias effects from address aliasing.
Analyzing an example with bias to enviornment size, we determined that collisions between automatic variables and static data resulted in aliasing for certain stack positions.
Aliasing conditions were triggered because variations in environment size offset the virtual addresses of stack allocated variables.
We show how aliasing can have significant performance impact in practice for algorithms operating on heap allocated buffers, analyzing and example with more than 50 \% performance variation between different memory layouts.
Bacause typical heap allocators will return aliased pointers for large allocations, worst case performance is likely to be the default.

We show how techniques like padding of variables and alternative alias-free code paths can be used to avoid aliasing at runtime.
For heap address aliasing especially, we find that manual intervention is sometimes required in order to achieve optimal performance.
Our results should inspire more clever optimizations and heuristics taking address aliasing effects into account, as the potential performance improvements can be substantial. 

% Acknowledgements
\begin{acks}
This work is forked from a Master's thesis project under the supervision of Anne Cathrine Elster and Rune Erlend Jensen, looking at sources of measurment bias on the Intel Ivy Bridge architecture~\cite{MasterThesis}.
\end{acks}

% Bibliography
\bibliographystyle{ACM-Reference-Format-Journals}
\bibliography{references}

% History dates
\received{}{}{}

\end{document}
